plan for implementation of pathology data model


Phase I. Skimming
Every time SPT library code successfully parses a data item, it tries to push that data item to the RDS database.
It should have a relatively liberal policy about guessing correct values when not explicitly supplied (e.g. study names), while leaving entries blank if the correct values can likely be uncovered eventually.

Phase II. Option to source from database
Individual modules or functions of SPT will attempt to source their inputs from the database. Will need study names to decide which data to pull.,

Phase III. Separation of ETL and SPT library core
Remove the input data parsing (Extract/Transform/Load), which involves vendor-specific idiosyncracies (HALO), from the SPT library core. Ideally the system of ETL scripts is extensible to cover alternative input modes, all targeting the same regularized format.


Table-by-table notes on Phase I implementation
- Subject. Probably just identifier for now. Skim at time of parsing outcomes file.
- Diagnosis. Disease probably empty. Result populated with outcomes strings. Skim at time of parsing outcomes file.
- Specimen collection study. All blank for now except name, autogenerated from file manifest project field.
- Specimen collection process. One for each slide, I guess associated 1-1 with HALO source files themselves. Use modification of source identifier.. the patient identifier, if possible to easily skim from diagnosis csv. Study the one guessed at the beginning of the pipeline. Other stuff empty for now.
- Histology assessment process. Not sure if these are always available. Currently will be blank.
- Specimen data measurement study. Again make this up for the pipeline run based on the project tokens in the file manifest. And if "HALO software cell manifest" is in the list of file data types, can use this to infer basic assay / machine / software values.
- Biological marking system. Add a 'marking mechanism' config entry, give it a default value mxIF for the time being (given that the workflows state this anyway). Study is the one just guessed. Target/antibody pairs ... Target coming from elementary phenotypes... antibody name for the time being guessed/inferred from this.
- Specimen data measurement process. Make one for each cell manifest source file. Try to trace it to the patient identifier... Whatever you do in specimen collection process. Study is the one just made.
- Plane coordinate reference system. Need more information, leave blank.
- Data file. Create per cell manifest. Everything should be available for this.
- Histological structure identification. Assume cells from "cell manifests". Assume... something else from compartments. Create uuids for the histological structure, with "cell" or "<region name?>". Create a SHP file with a closed polygon of the bounding box. Since "HALO software cell manifest" is there, can create rudimentary values for identification method ("segmentation in HALO" or something). Slide is same thing you made up in specimen collection process.
- Histological structure. See histological structure identification.
- Shape file. Use SHP and the file contents created in histological structure identification.
- Cell phenotype. Read off from complex phenotypes.
- Cell phenotype criterion. Read off from complex phenotypes. Study is data analysis study made up for this session ... or input data? Project name etc.
- Chemical species. Read off from elementary phenotypes. Including chemical structure class... though Rami doesn't seem to be using this.
- Expression quantification. One for each histological structure which is a cell in a cell manifest ... nucleus/cyto/membrane if available, and each parseable column that can be associated with a chemical species (elementary phenotype). Leave quantification method blank for now. Unit inferred. Discrete value if available in "Positive" column.
- Feature specification. Basically one for each row that comes out of the outcomes testing.
- Feature specifier. Just list the (usually phenotypes... also compartment?) tokens that differentiate the test conditions.
- Diagnostic selection criterion. Easily available, except possibly disease.
- Two-cohort feature association test. Everything is available



General-purpose db-related behaviors:
- For a given table, count how many records are part of a given study. Mainly to check for 0, so bulk-load is sanctioned.
- Complete a set of records to include automatically generated identifiers. E.g. consecutive integers starting from the maximum value of the existing identifiers, in case they will always be integers ... or even just the maximum of the ones which *are* integers ... Is auto-increment available on a per-query basis? Doesn't look like it.
- Bulk load a set of (complete) records. Use pagination to overcome possible dropped-packets-style errors. Use python standard library mmap file, and psycopg2 (the postgres driver for Python) copy_to(file, tablename).
- 

