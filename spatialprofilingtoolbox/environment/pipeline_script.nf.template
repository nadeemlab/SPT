
nextflow.enable.dsl=2

config_filename="spt-print config-filename".execute().text.trim()
config_file_ch = channel.fromPath(config_filename)

intermediate_database_filename="spt-print intermediate-database-filename".execute().text.trim()
intermediate_database_file_ch = channel.fromPath(intermediate_database_filename)

process generate_jobs {
    input:
    path config_file

    output:
    path 'job_specification_table.tsv'

    script:
    """
    spt-pipeline generate-jobs --job-specification-table=job_specification_table.tsv
    """
}

process retrieve_job_specifiers {
    input:
    path job_specification_table

    output:
    stdout emit: x
    // tokens to a channel for which token pushing leads to process forks

    script:
    """
    cat "Contents of job specification table just generated:"
    cat $job_specification_table
    """
    // just echo the job specifiers found in the source file to the output channel as individual tokens
}

// process single_job {
//     input:
//     the job specifier channel

//     output:
//     intermediate database file

//     script:
//     spt-pipeline single-job
// }

// process intermediate_database_merging {
//     input:
//     a huge long list of positional arguments representing the files just generated by all the single jobs

//     output:
//     same named intermediate database file, just that its one of them

//     script:
//     spt-pipeline merge-sqlite-dbs all those positional args --output={{intermediate database file}}
// }

// process aggregate_results {
//     input:
//     a huge long list of positional arguments

//     output:
//     the single known (previously intermediate) database filename, and also the stats filename and whatever else the workflow stipulates

//     script:
//     spt-pipeline aggregate-results
// }

workflow {
    retrieve_job_specifiers(generate_jobs(config_file_ch))
    retrieve_job_specifiers.out.x.view()
}

