#!/usr/bin/env python3
import argparse
import os
from os.path import exists
from os.path import join
from os.path import basename

import spatialprofilingtoolbox
from spatialprofilingtoolbox.workflows import workflows
from spatialprofilingtoolbox.workflows import workflow_names

if __name__=='__main__':
    parser = argparse.ArgumentParser(
        description='''
        Create a list of core, parallelizable job specifications for a given SPT
        workflow, as well as lists of file dependencies.
        
        Note: Due to orchestration design constraints, if this script must
        depend on file contents, it can *only* depend on the contents of explicitly
        indicated files. That is, it cannot "bootstrap" and open files whose names
        are discovered by reading other files' contents.
        ''',
    )
    parser.add_argument(
        '--workflow',
        dest='workflow',
        type=str,
        choices=workflow_names,
        required=True,
    )
    parser.add_argument(
        '--file-manifest-file',
        dest='file_manifest_file',
        type=str,
        required=True,
        help='''
        Path to the file manifest file. If just a file basename, it is presumed to be
        in the current working directory, since this script is presumed to be
        deployed as a Nextflow process.
        ''',
    )
    parser.add_argument(
        '--input-path',
        dest='input_path',
        type=str,
        required=True,
        help='''Path to directory containing input data files. (For example,
        containing file_manifest.tsv).
        ''',
    )
    parser.add_argument(
        '--outcomes-file',
        dest='outcomes_file',
        type=str,
        required=True,
        help='File containing outcome assignments to Sample ID values.',
    )
    parser.add_argument(
        '--job-specification-table',
        dest='job_specification_table',
        type=str,
        required=True,
        help='Filename for output, job specification table CSV.',
    )
    parser.add_argument(
        '--dataset-metadata-files-list-file',
        dest='dataset_metadata_files_list_file',
        type=str,
        required=True,
        help='Filename for output, list of dataset metadata filenames.',
    )
    parser.add_argument(
        '--elementary-phenotypes-filename',
        dest='elementary_phenotypes_filename',
        type=str,
        required=True,
        help='Filename for output, the elementary phenotypes filename.',
    )
    parser.add_argument(
        '--composite-phenotypes-filename',
        dest='composite_phenotypes_filename',
        type=str,
        required=True,
        help='Filename for output, the composite phenotypes filename.',
    )

    # parser.add_argument(
    #     '--outcomes-filename',
    #     dest='outcomes_filename',
    #     type=str,
    #     required=True,
    #     help='Filename for output, the composite phenotypes filename.',
    # )

    args = parser.parse_args()

    if not exists(args.file_manifest_file):
        raise FileNotFoundError(args.file_manifest_file)

    Generator = workflows[args.workflow].generator
    DatasetDesign = workflows[args.workflow].dataset_design
    job_generator = Generator(
        file_manifest_file=args.file_manifest_file,
        input_path=args.input_path,
        dataset_design_class=DatasetDesign,
    )
    job_generator.write_job_specification_table(args.job_specification_table, args.outcomes_file)
    job_generator.write_dataset_metadata_files_list(args.dataset_metadata_files_list_file)
    job_generator.write_elementary_phenotypes_filename(args.elementary_phenotypes_filename)
    job_generator.write_composite_phenotypes_filename(args.composite_phenotypes_filename)
