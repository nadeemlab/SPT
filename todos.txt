
0. [DONE] Try separating the counts and counts_all... so counts_all happens firsts? Problem is that the finishedness of counts in most (subsequent) cases is triggering a possible loop in notification processing.
  - [DONE] Possibly try making the cell data puller allow a cell identifier filter... make all this way easier.


1. Multiple feature specs created for same feature, observed in case of no markers when the denominator for those counts features.
  - Come up with a recipe for reproduction of the bug.
  - Monitor the check for existing feature.
  * This bug seems like it is hiding something other bug, because the logic is very simple.
  - Possibly related: Why is there always a loop, Waiting for signals... Received signal that whole feature ... ?
2. The watcher is failing to observe the initial 10 or so examples of worker job taking; hint in the fact that the logs generated by the workers that the activity was noticed are all out of order, but the logs generated by the workers for job taking are conspiciously in simple order.
  - Known, related incorrect behavior is the fact that the watcher uses a dict with PID as the key, when in fact the same PID (worker) can be associated with multiple jobs... at least, in a row...
3. Create parallel counts endpoint that does NOT use any of the crazy queue logic.
  - For each request for feature, just request 1 occurrence of worker job taking for each so-far-uncomputed value, unless there are already jobs queued, then do nothing. This ought to be easy enough to implement simply by making the ..._queue table have a constraint and do inserts with "on conflict do nothing".
  - This will eventually allow deprecating of the mass-nulling behavior.
4. Add a cancel metrics endpoint... or option on each of the feature request endpoints. The effect will be to remove all the associated jobs from the queue.
5. Add link for GNN plots.
6. Merge plugin stuff.
7. Try the run_all script, monitoring the backend for new bugs.
  - Make sure same results are found in UI (at least after run_all).
 



... Example logs highlighting a timing issue:

|ondemand2  90 ondemand.worker |                                                    Wk says: (276) queue_pop_activity "3	Melanoma intralesional IL2	lesion 0_1"
|ondemand-  90 ondemand.worker |                                                    Wk says: (275) queue_pop_activity "3	Melanoma intralesional IL2	lesion 0_2"
|ondemand2 ondemand.worker     | 276 doing job (feature=3, sample=lesion 0_1)
|watcher--  42 watcher.watcher |                                                             (276) queue_pop_activity "3	Melanoma intralesional IL2	lesion 0_1" sees watcher
|watcher-- watcher.watcher     | 276 claims to be working on 3 lesion 0_1.
|watcher-- 132 watcher.watcher |                                                    WW says:      queue_failed_jobs_cleared
|ondemand- ondemand.worker     | 275 doing job (feature=3, sample=lesion 0_2)
|watcher-- watcher.watcher     | 1 workers actively working on jobs (276).
|ondemand- 101 ondemand.worker |                                                    Wk says: (275) queue_job_complete "3	Melanoma intralesional IL2	lesion 0_2"
|ondemand2 101 ondemand.worker |                                                    Wk says: (276) queue_job_complete "3	Melanoma intralesional IL2	lesion 0_1"
|watcher--  42 watcher.watcher |                                                             (275) queue_job_complete "3	Melanoma intralesional IL2	lesion 0_2" sees watcher
|watcher--  80 watcher.watcher | Neither worker 275 nor job 3, lesion 0_2 are recorded. Completion notice ignored.
|watcher-- 154 watcher.watcher â”ƒ Completion of job "3	Melanoma intralesional IL2	lesion 0_1" by 276 inferred, direct notice absent.

276 = Worker1  0_1 = Job A
275 = Worker2  0_2 = Job B

In summary:

1.  Worker1 picks up Job A.
2.  Worker2 picks up Job B.
3.  Watcher records (1).
4.  Watcher sends heartbeat signal.
5.  Watcher summarizes: 1 active job (according to him, namely Job A by Worker 1).
6.  Worker2 completes Job B.
7.  Worker1 completes Job A.
8.  Watcher confused about (6), not having seen message (2) yet somehow.
9.  Watcher, still processing message (6), prematurely looks for "failed jobs", before having processed its notification queue, and believes Worker1's Job A to be complete but without notice (in fact, the notice is in the following message, (7)).

The first wrong event seems to be (8). Why was message (6) seen by the Watcher before message (2)?



* Add a special case for counts to make it faster... just report the array length.
* Move notice of possible feature completion to end of pickup-compute loop on each container ... modulated by change in feature? So it's not waiting forever, after feature is actually done
* Null insertion needs to be an option in the counts case... in importance counts, some don't exist.
* Does the notification generator block the API instance? Seems like can't access sometimes
* Bug: ex. "Adipo..." in Mel-intr: On second try, full computation of pop. fractions was performed twice, got multiple features.

* Need related cell set cache to be deleted when ondemand features are dropped!
* Null needs to be allowed as a final return value in gnn counts case
* Add link to plot
* Two classes of service ... low ram and high. Jobs can pick up based on classification of studies, or samples?

* Check that the max integer histological structure id is the right cell count... 
* Remove "on delete cascade" from new tables
